# Recap on McKinsey’s proposal

From our [last article](https://open.substack.com/pub/mvtechstack/p/is-
mckinsey-right-on-developer-
productivity?r=iqxi6&utm_campaign=post&utm_medium=web), we went through bits
and pieces of [McKinsey’s proposal on measuring developer
productivity](https://www.mckinsey.com/industries/technology-media-and-
telecommunications/our-insights/yes-you-can-measure-software-developer-
productivity). Authors from McKinsey developed an approach using surveys and
existing data from tools like JIRA to make measuring software developer
productivity more feasible. By gathering data at individual, system and
business levels, it provides a holistic view to identify improvement
opportunities. They argue that, business outcomes reflect the impact,
including a 20-30% reduction in defects, 20% better employee experience, and
60 percentage points higher customer satisfaction.

In this article, let me try to dissect the article and provide some ideas on
how our tech leaders and business owners can take to check the health of the
tech project they are managing or sponsoring.

# Blind spots in McKinsey’s thinking

## Bad source of data

[![garbage in, garbage out.](https://substack-post-
media.s3.amazonaws.com/public/images/4829eb1a-8146-49b9-accb-056b0543faa6_1024x1024.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-
post-
media.s3.amazonaws.com%2Fpublic%2Fimages%2F4829eb1a-8146-49b9-accb-056b0543faa6_1024x1024.jpeg)

McKinsey proposes JIRA or other product development backlog system and survey
as a unit of data to visualize developer productivity. While JIRA and survey
provide some useful data, relying on it for measuring developer productivity
has a huge limitation — **data quality**. JIRA usage can be inconsistent, vary
between teams and the context is missing in those ticketing system.
Furthermore, it is not directly the output of the software. Imagine without
the project management tool, can a software team iterate the software? Yes.
This thought experiment reveals a truth — tickets in your project management
tool are a proxy of work. It is less ideal when you have a better way to
generate a useful metric — your codebase.

## Unclear cause and effect on productivity metrics

[![](https://substack-post-
media.s3.amazonaws.com/public/images/6813ab86-ba05-4cb0-acaa-07de366aeaff_918x370.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-
post-
media.s3.amazonaws.com%2Fpublic%2Fimages%2F6813ab86-ba05-4cb0-acaa-07de366aeaff_918x370.png)correlation
vs causation

McKinsey chooses customer perceived defects, employee experience and customer
satisfaction as the business impact metrics. The article claims it increases
by a wide margin after adopting their proposed developer productivity metrics.
But can the authors casually skim through the relationship between the metrics
and the business impact. I have no problem on recognizing them to be a part of
the reason, but is there a big correlation? Is there a cause and effect
relationship? The best we can say that it is not stated and studied.

There is also subjectivity in the metrics themselves. Employee satisfaction
stands out as the most obvious one. All sizes of HR policies, from RTO policy
down to increasing or cutting office snack supplies, can affect the employee
satisfaction and intervene with the experiments.

## Encourage conformance and cargo cult in technical decisions

I particularly have a concern with DVI, which compares the companies'
technological stack and other related matters, such as talent management, with
the peers in the market. Each company should take these decision seriously and
apply a first principle approach to choose their own tech stack. But that’s
another post, another time.

# Let’s define developer productivity

[![](https://substack-post-
media.s3.amazonaws.com/public/images/f87fc8ce-5dc5-4992-80c4-e48a1b9fc31d_1920x1080.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-
post-
media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff87fc8ce-5dc5-4992-80c4-e48a1b9fc31d_1920x1080.jpeg)Velocity
= speed x direction

This simple formula captures two important aspects of productivity—the rate of
progress as well as the value of the result. Speed alone does not necessarily
translate to productivity. If a developer is working quickly but in an aimless
or misguided manner, their effort is wasted. Likewise, having a clear
direction is worthless if no forward momentum is being made. True productivity
balances both acceleration and alignment. Speed must be aimed appropriately to
move the business or technical objectives forward. And direction requires
regular checkpointing to maintain a fast enough velocity. This framework
reminds us to consider both how efficiently work is getting done, and whether
the work itself is worthwhile. Monitoring and improving on both speed and
direction leads to optimal developer productivity over time.

# What to measure instead

Throughout my journey as a tech lead and product lead, I am always looking out
for some metrics that evaluate my performance. Here are some ideas you may
want to ponder and give you ideas of how to design the right metrics for you.

## Number of unit of work that needs rework before shipping

[![requirements are hard.](https://substack-post-
media.s3.amazonaws.com/public/images/f66cbe7b-e1b8-4a39-8b71-5074ef572330_704x528.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-
post-
media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff66cbe7b-e1b8-4a39-8b71-5074ef572330_704x528.jpeg)Requirements
are hard.

Frequent rework points to inefficiencies and unnecessary repetition in a
developer's work. It often stems from high code churn—unstable or low-quality
code that is constantly being changed, broken and fixed again. This metric
provides insight into problems like inadequate testing, lack of automation, or
failure to refactor technical debt. Reducing rework frees up developer
capacity. With less time spent fixing previous errors and more focus on
forward progress, code quality and velocity increase overall. Monitoring both
churn and rework metrics together helps identify optimization opportunities in
workflow practices.

## Number of Relationship between modules/microservices

[![A typical computer room in the 00s with messy cables.](https://substack-
post-
media.s3.amazonaws.com/public/images/0fb49344-4919-49b3-9b09-bfaf38011e7c_1015x761.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-
post-
media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fb49344-4919-49b3-9b09-bfaf38011e7c_1015x761.jpeg)Visualization
of a microservices based system in a startup with 0 revenue.

The number of relationships or dependencies that exist between software
modules or microservices can be a useful productivity metric, as it reflects
the overall cognitive complexity and cohesion of the system architecture. A
high level of interdependency indicates tight coupling, which makes code
brittle and harder to change. As developers spend more time navigating and
working around relationships, productivity decreases. Tracking this metric
over time can reveal where strategic refactoring may loosen constraints and
simplify development. It also helps determine when to split or merge code as
needed. Architectural choices that minimize superfluous relationships empower
developers with greater flexibility and autonomy in their work.

## Hotspot of changes

Tracking hotspots over time provides valuable insights into developer
productivity and where opportunities may exist. For example, if a developer is
repeatedly making tweaks to the same section of code, it could indicate a need
for refactoring to improve maintainability. Alternatively, frequent changes
spread across many areas might reflect inefficient practices or lack of
architectural stability. As a team, viewing hotspot data can help identify
patterns that are helping or hindering workflow. Developers may also learn
from each other about more effective ways to structure code for long-term
productivity gains.

# Rule of thumbs in deciding what to measure

The above three metrics are just suggestions. But how should you construct
your own? What should be the principles that I adhere to?

## Close to the metal

The above metrics are usually from the codebase and system itself. Since the
software IS the work that we are all working on, it is the result and what our
customers and users will be interacting with. Measurements from there are one
of the most direct feedback we can get from the customers.

## Clarity

The metrics chosen should be clearly defined and understandable for both
developers and leadership. Vague or ambiguous goals will likely lead to
frustration or gaming of the system rather than improved performance. Take
time upfront to discuss the why and how behind any metrics.

## Move business levers

Focus on tracking outputs that directly correlate with and drive important
business objectives. For a software company, this may include user retention
or new user personas. Avoid vanity metrics divorced from reality or outside a
developer's control. Or state clearly there are other factors that affect the
business performance besides developers’ work.

## Measure and improve

Establish benchmarks, collect data over time, and review periodically with
developers to identify areas for optimization. But don't over-index on
measurement alone. Create space for experimentation and learning. The goal
should be continual improvement, not judgement. Metrics work best when they
inform constructive discussions about problems to solve. Judgement is the last
resort of managing the team.

# Conclusion

While McKinsey's goal of providing transparency to non-technical leaders was
well-intentioned, their approach to measuring developer productivity missed
the mark. As experts in business consulting, they understandably viewed
developers through a traditional output-focused lens. However, software
development is a creative and complex endeavor that can't be fully captured by
surface metrics alone.

By remaining at a high level rather than engaging with software practitioners,
McKinsey failed to gain the necessary context into developers’ work.
Productivity is influenced by many interlocking factors beyond developers’
control, like shifting priorities and technical roadblocks. Rather than judge,
leaders need to first understand dev teams’ challenges to best support them.

Through this article, I brought the leaders down from the clouds and into
reality with the developers. Let's have an honest discussion about what really
drives our ability to deliver value, where processes can improve, and how job
satisfaction impacts productivity. If we address these foundational issues
together, I believe we'll find the true drivers of sustainable output over the
long run.

Need technical leadership grounded, not air headed? Hire tech leads who write
more lines of code than slings buzzwords.

