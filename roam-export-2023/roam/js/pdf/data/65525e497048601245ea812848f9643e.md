- Version: 2.0
- https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FChaChaanTengv3%2F-mFk8LwF-D.pdf?alt=media&token=4d69e749-d0cc-4c4b-8f64-789014fad04f
    - `{"uid":"3oMDCNNJ9","resumePage":1,"versionName":" ","alreadyImported":true}`
        - `{"id":"d14ce7ee-28c8-8d69-fca0-74c4471641e7","time":"2023-09-21T03:01:34.964Z","repliedToTextUid":"","commentTextUid":"","textUid":"jyqhRTq9t","dataUid":"priiBH_J7","textBlockExist":"","author":"Gilbert Wat","annotType":"Highlight","pageNumber":1,"color":"ffcd45ff","text":"The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely.","xfdf":"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<xfdf xmlns=\"http://ns.adobe.com/xfdf/\" xml:space=\"preserve\">\n<fields />\n<add><highlight page=\"0\" rect=\"143.557,314.318,469.381,380.240\" color=\"#FFCD45\" flags=\"print\" name=\"d14ce7ee-28c8-8d69-fca0-74c4471641e7\" title=\"Gilbert Wat\" subject=\"Highlight\" date=\"D:20230921110134+08'00'\" creationdate=\"D:20230921110125+08'00'\" coords=\"143.55700000000002,380.24044239999995,468.3196032587912,380.24044239999995,143.55700000000002,368.86346453125,468.3196032587912,368.86346453125,143.866,369.33144239999996,468.14077482068956,369.33144239999996,143.866,357.95446453125,468.14077482068956,357.95446453125,143.866,358.42244239999997,468.13069731999997,358.42244239999997,143.866,347.04546453125,468.13069731999997,347.04546453125,143.866,347.5134424,469.3806051160002,347.5134424,143.866,336.13646453125,469.3806051160002,336.13646453125,143.866,336.6044424,468.1360771239998,336.6044424,143.866,325.22746453125,468.1360771239998,325.22746453125,143.866,325.69544239999993,176.80056233200003,325.69544239999993,143.866,314.31846453124996,176.80056233200003,314.31846453124996\"><trn-custom-data bytes=\"{&quot;trn-annot-preview&quot;:&quot;The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.&quot;,&quot;dataUid&quot;:&quot;priiBH_J7&quot;,&quot;textUid&quot;:&quot;jyqhRTq9t&quot;}\"/></highlight></add>\n<modify />\n<delete />\n</xfdf>"}`
            -  #h:BrightSun#h:ffcd45ff#[[c3-pdf-highlight]] The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. {{1}}{{üìã}}{{‚ùå}}{{üí¨}}
